# This file was auto-generated by Fern from our API Definition.

import datetime as dt
import typing
from json.decoder import JSONDecodeError

from ..core.api_error import ApiError
from ..core.client_wrapper import AsyncClientWrapper, SyncClientWrapper
from ..core.datetime_utils import serialize_datetime
from ..core.http_response import AsyncHttpResponse, HttpResponse
from ..core.jsonable_encoder import jsonable_encoder
from ..core.request_options import RequestOptions
from ..core.serialization import convert_and_respect_annotation_metadata
from ..core.unchecked_base_model import construct_type
from ..types.analysis_plan import AnalysisPlan
from ..types.artifact_plan import ArtifactPlan
from ..types.assistant import Assistant
from ..types.compliance_plan import CompliancePlan
from ..types.create_assistant_dto_background_sound import CreateAssistantDtoBackgroundSound
from ..types.create_assistant_dto_client_messages_item import CreateAssistantDtoClientMessagesItem
from ..types.create_assistant_dto_credentials_item import CreateAssistantDtoCredentialsItem
from ..types.create_assistant_dto_first_message_mode import CreateAssistantDtoFirstMessageMode
from ..types.create_assistant_dto_hooks_item import CreateAssistantDtoHooksItem
from ..types.create_assistant_dto_model import CreateAssistantDtoModel
from ..types.create_assistant_dto_server_messages_item import CreateAssistantDtoServerMessagesItem
from ..types.create_assistant_dto_transcriber import CreateAssistantDtoTranscriber
from ..types.create_assistant_dto_voice import CreateAssistantDtoVoice
from ..types.create_assistant_dto_voicemail_detection import CreateAssistantDtoVoicemailDetection
from ..types.keypad_input_plan import KeypadInputPlan
from ..types.langfuse_observability_plan import LangfuseObservabilityPlan
from ..types.message_plan import MessagePlan
from ..types.monitor_plan import MonitorPlan
from ..types.server import Server
from ..types.start_speaking_plan import StartSpeakingPlan
from ..types.stop_speaking_plan import StopSpeakingPlan
from ..types.transport_configuration_twilio import TransportConfigurationTwilio
from .types.update_assistant_dto_background_sound import UpdateAssistantDtoBackgroundSound
from .types.update_assistant_dto_client_messages_item import UpdateAssistantDtoClientMessagesItem
from .types.update_assistant_dto_credentials_item import UpdateAssistantDtoCredentialsItem
from .types.update_assistant_dto_first_message_mode import UpdateAssistantDtoFirstMessageMode
from .types.update_assistant_dto_hooks_item import UpdateAssistantDtoHooksItem
from .types.update_assistant_dto_model import UpdateAssistantDtoModel
from .types.update_assistant_dto_server_messages_item import UpdateAssistantDtoServerMessagesItem
from .types.update_assistant_dto_transcriber import UpdateAssistantDtoTranscriber
from .types.update_assistant_dto_voice import UpdateAssistantDtoVoice
from .types.update_assistant_dto_voicemail_detection import UpdateAssistantDtoVoicemailDetection

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class RawAssistantsClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._client_wrapper = client_wrapper

    def list(
        self,
        *,
        limit: typing.Optional[float] = None,
        created_at_gt: typing.Optional[dt.datetime] = None,
        created_at_lt: typing.Optional[dt.datetime] = None,
        created_at_ge: typing.Optional[dt.datetime] = None,
        created_at_le: typing.Optional[dt.datetime] = None,
        updated_at_gt: typing.Optional[dt.datetime] = None,
        updated_at_lt: typing.Optional[dt.datetime] = None,
        updated_at_ge: typing.Optional[dt.datetime] = None,
        updated_at_le: typing.Optional[dt.datetime] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> HttpResponse[typing.List[Assistant]]:
        """
        Parameters
        ----------
        limit : typing.Optional[float]
            This is the maximum number of items to return. Defaults to 100.

        created_at_gt : typing.Optional[dt.datetime]
            This will return items where the createdAt is greater than the specified value.

        created_at_lt : typing.Optional[dt.datetime]
            This will return items where the createdAt is less than the specified value.

        created_at_ge : typing.Optional[dt.datetime]
            This will return items where the createdAt is greater than or equal to the specified value.

        created_at_le : typing.Optional[dt.datetime]
            This will return items where the createdAt is less than or equal to the specified value.

        updated_at_gt : typing.Optional[dt.datetime]
            This will return items where the updatedAt is greater than the specified value.

        updated_at_lt : typing.Optional[dt.datetime]
            This will return items where the updatedAt is less than the specified value.

        updated_at_ge : typing.Optional[dt.datetime]
            This will return items where the updatedAt is greater than or equal to the specified value.

        updated_at_le : typing.Optional[dt.datetime]
            This will return items where the updatedAt is less than or equal to the specified value.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[typing.List[Assistant]]

        """
        _response = self._client_wrapper.httpx_client.request(
            "assistant",
            method="GET",
            params={
                "limit": limit,
                "createdAtGt": serialize_datetime(created_at_gt) if created_at_gt is not None else None,
                "createdAtLt": serialize_datetime(created_at_lt) if created_at_lt is not None else None,
                "createdAtGe": serialize_datetime(created_at_ge) if created_at_ge is not None else None,
                "createdAtLe": serialize_datetime(created_at_le) if created_at_le is not None else None,
                "updatedAtGt": serialize_datetime(updated_at_gt) if updated_at_gt is not None else None,
                "updatedAtLt": serialize_datetime(updated_at_lt) if updated_at_lt is not None else None,
                "updatedAtGe": serialize_datetime(updated_at_ge) if updated_at_ge is not None else None,
                "updatedAtLe": serialize_datetime(updated_at_le) if updated_at_le is not None else None,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    typing.List[Assistant],
                    construct_type(
                        type_=typing.List[Assistant],  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)

    def create(
        self,
        *,
        transcriber: typing.Optional[CreateAssistantDtoTranscriber] = OMIT,
        model: typing.Optional[CreateAssistantDtoModel] = OMIT,
        voice: typing.Optional[CreateAssistantDtoVoice] = OMIT,
        first_message: typing.Optional[str] = OMIT,
        first_message_interruptions_enabled: typing.Optional[bool] = OMIT,
        first_message_mode: typing.Optional[CreateAssistantDtoFirstMessageMode] = OMIT,
        voicemail_detection: typing.Optional[CreateAssistantDtoVoicemailDetection] = OMIT,
        client_messages: typing.Optional[typing.Sequence[CreateAssistantDtoClientMessagesItem]] = OMIT,
        server_messages: typing.Optional[typing.Sequence[CreateAssistantDtoServerMessagesItem]] = OMIT,
        silence_timeout_seconds: typing.Optional[float] = OMIT,
        max_duration_seconds: typing.Optional[float] = OMIT,
        background_sound: typing.Optional[CreateAssistantDtoBackgroundSound] = OMIT,
        background_denoising_enabled: typing.Optional[bool] = OMIT,
        model_output_in_messages_enabled: typing.Optional[bool] = OMIT,
        transport_configurations: typing.Optional[typing.Sequence[TransportConfigurationTwilio]] = OMIT,
        observability_plan: typing.Optional[LangfuseObservabilityPlan] = OMIT,
        credentials: typing.Optional[typing.Sequence[CreateAssistantDtoCredentialsItem]] = OMIT,
        hooks: typing.Optional[typing.Sequence[CreateAssistantDtoHooksItem]] = OMIT,
        name: typing.Optional[str] = OMIT,
        voicemail_message: typing.Optional[str] = OMIT,
        end_call_message: typing.Optional[str] = OMIT,
        end_call_phrases: typing.Optional[typing.Sequence[str]] = OMIT,
        compliance_plan: typing.Optional[CompliancePlan] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        analysis_plan: typing.Optional[AnalysisPlan] = OMIT,
        artifact_plan: typing.Optional[ArtifactPlan] = OMIT,
        message_plan: typing.Optional[MessagePlan] = OMIT,
        start_speaking_plan: typing.Optional[StartSpeakingPlan] = OMIT,
        stop_speaking_plan: typing.Optional[StopSpeakingPlan] = OMIT,
        monitor_plan: typing.Optional[MonitorPlan] = OMIT,
        credential_ids: typing.Optional[typing.Sequence[str]] = OMIT,
        server: typing.Optional[Server] = OMIT,
        keypad_input_plan: typing.Optional[KeypadInputPlan] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> HttpResponse[Assistant]:
        """
        Parameters
        ----------
        transcriber : typing.Optional[CreateAssistantDtoTranscriber]
            These are the options for the assistant's transcriber.

        model : typing.Optional[CreateAssistantDtoModel]
            These are the options for the assistant's LLM.

        voice : typing.Optional[CreateAssistantDtoVoice]
            These are the options for the assistant's voice.

        first_message : typing.Optional[str]
            This is the first message that the assistant will say. This can also be a URL to a containerized audio file (mp3, wav, etc.).

            If unspecified, assistant will wait for user to speak and use the model to respond once they speak.

        first_message_interruptions_enabled : typing.Optional[bool]

        first_message_mode : typing.Optional[CreateAssistantDtoFirstMessageMode]
            This is the mode for the first message. Default is 'assistant-speaks-first'.

            Use:
            - 'assistant-speaks-first' to have the assistant speak first.
            - 'assistant-waits-for-user' to have the assistant wait for the user to speak first.
            - 'assistant-speaks-first-with-model-generated-message' to have the assistant speak first with a message generated by the model based on the conversation state. (`assistant.model.messages` at call start, `call.messages` at squad transfer points).

            @default 'assistant-speaks-first'

        voicemail_detection : typing.Optional[CreateAssistantDtoVoicemailDetection]
            These are the settings to configure or disable voicemail detection. Alternatively, voicemail detection can be configured using the model.tools=[VoicemailTool].
            This uses Twilio's built-in detection while the VoicemailTool relies on the model to detect if a voicemail was reached.
            You can use neither of them, one of them, or both of them. By default, Twilio built-in detection is enabled while VoicemailTool is not.

        client_messages : typing.Optional[typing.Sequence[CreateAssistantDtoClientMessagesItem]]
            These are the messages that will be sent to your Client SDKs. Default is conversation-update,function-call,hang,model-output,speech-update,status-update,transfer-update,transcript,tool-calls,user-interrupted,voice-input,workflow.node.started. You can check the shape of the messages in ClientMessage schema.

        server_messages : typing.Optional[typing.Sequence[CreateAssistantDtoServerMessagesItem]]
            These are the messages that will be sent to your Server URL. Default is conversation-update,end-of-call-report,function-call,hang,speech-update,status-update,tool-calls,transfer-destination-request,user-interrupted. You can check the shape of the messages in ServerMessage schema.

        silence_timeout_seconds : typing.Optional[float]
            How many seconds of silence to wait before ending the call. Defaults to 30.

            @default 30

        max_duration_seconds : typing.Optional[float]
            This is the maximum number of seconds that the call will last. When the call reaches this duration, it will be ended.

            @default 600 (10 minutes)

        background_sound : typing.Optional[CreateAssistantDtoBackgroundSound]
            This is the background sound in the call. Default for phone calls is 'office' and default for web calls is 'off'.
            You can also provide a custom sound by providing a URL to an audio file.

        background_denoising_enabled : typing.Optional[bool]
            This enables filtering of noise and background speech while the user is talking.

            Default `false` while in beta.

            @default false

        model_output_in_messages_enabled : typing.Optional[bool]
            This determines whether the model's output is used in conversation history rather than the transcription of assistant's speech.

            Default `false` while in beta.

            @default false

        transport_configurations : typing.Optional[typing.Sequence[TransportConfigurationTwilio]]
            These are the configurations to be passed to the transport providers of assistant's calls, like Twilio. You can store multiple configurations for different transport providers. For a call, only the configuration matching the call transport provider is used.

        observability_plan : typing.Optional[LangfuseObservabilityPlan]
            This is the plan for observability configuration of assistant's calls.
            Currently supports Langfuse for tracing and monitoring.

        credentials : typing.Optional[typing.Sequence[CreateAssistantDtoCredentialsItem]]
            These are dynamic credentials that will be used for the assistant calls. By default, all the credentials are available for use in the call but you can supplement an additional credentials using this. Dynamic credentials override existing credentials.

        hooks : typing.Optional[typing.Sequence[CreateAssistantDtoHooksItem]]
            This is a set of actions that will be performed on certain events.

        name : typing.Optional[str]
            This is the name of the assistant.

            This is required when you want to transfer between assistants in a call.

        voicemail_message : typing.Optional[str]
            This is the message that the assistant will say if the call is forwarded to voicemail.

            If unspecified, it will hang up.

        end_call_message : typing.Optional[str]
            This is the message that the assistant will say if it ends the call.

            If unspecified, it will hang up without saying anything.

        end_call_phrases : typing.Optional[typing.Sequence[str]]
            This list contains phrases that, if spoken by the assistant, will trigger the call to be hung up. Case insensitive.

        compliance_plan : typing.Optional[CompliancePlan]

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            This is for metadata you want to store on the assistant.

        analysis_plan : typing.Optional[AnalysisPlan]
            This is the plan for analysis of assistant's calls. Stored in `call.analysis`.

        artifact_plan : typing.Optional[ArtifactPlan]
            This is the plan for artifacts generated during assistant's calls. Stored in `call.artifact`.

            Note: `recordingEnabled` is currently at the root level. It will be moved to `artifactPlan` in the future, but will remain backwards compatible.

        message_plan : typing.Optional[MessagePlan]
            This is the plan for static predefined messages that can be spoken by the assistant during the call, like `idleMessages`.

            Note: `firstMessage`, `voicemailMessage`, and `endCallMessage` are currently at the root level. They will be moved to `messagePlan` in the future, but will remain backwards compatible.

        start_speaking_plan : typing.Optional[StartSpeakingPlan]
            This is the plan for when the assistant should start talking.

            You should configure this if you're running into these issues:
            - The assistant is too slow to start talking after the customer is done speaking.
            - The assistant is too fast to start talking after the customer is done speaking.
            - The assistant is so fast that it's actually interrupting the customer.

        stop_speaking_plan : typing.Optional[StopSpeakingPlan]
            This is the plan for when assistant should stop talking on customer interruption.

            You should configure this if you're running into these issues:
            - The assistant is too slow to recognize customer's interruption.
            - The assistant is too fast to recognize customer's interruption.
            - The assistant is getting interrupted by phrases that are just acknowledgments.
            - The assistant is getting interrupted by background noises.
            - The assistant is not properly stopping -- it starts talking right after getting interrupted.

        monitor_plan : typing.Optional[MonitorPlan]
            This is the plan for real-time monitoring of the assistant's calls.

            Usage:
            - To enable live listening of the assistant's calls, set `monitorPlan.listenEnabled` to `true`.
            - To enable live control of the assistant's calls, set `monitorPlan.controlEnabled` to `true`.

            Note, `serverMessages`, `clientMessages`, `serverUrl` and `serverUrlSecret` are currently at the root level but will be moved to `monitorPlan` in the future. Will remain backwards compatible

        credential_ids : typing.Optional[typing.Sequence[str]]
            These are the credentials that will be used for the assistant calls. By default, all the credentials are available for use in the call but you can provide a subset using this.

        server : typing.Optional[Server]
            This is where Vapi will send webhooks. You can find all webhooks available along with their shape in ServerMessage schema.

            The order of precedence is:

            1. assistant.server.url
            2. phoneNumber.serverUrl
            3. org.serverUrl

        keypad_input_plan : typing.Optional[KeypadInputPlan]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[Assistant]

        """
        _response = self._client_wrapper.httpx_client.request(
            "assistant",
            method="POST",
            json={
                "transcriber": convert_and_respect_annotation_metadata(
                    object_=transcriber, annotation=CreateAssistantDtoTranscriber, direction="write"
                ),
                "model": convert_and_respect_annotation_metadata(
                    object_=model, annotation=CreateAssistantDtoModel, direction="write"
                ),
                "voice": convert_and_respect_annotation_metadata(
                    object_=voice, annotation=CreateAssistantDtoVoice, direction="write"
                ),
                "firstMessage": first_message,
                "firstMessageInterruptionsEnabled": first_message_interruptions_enabled,
                "firstMessageMode": first_message_mode,
                "voicemailDetection": convert_and_respect_annotation_metadata(
                    object_=voicemail_detection, annotation=CreateAssistantDtoVoicemailDetection, direction="write"
                ),
                "clientMessages": client_messages,
                "serverMessages": server_messages,
                "silenceTimeoutSeconds": silence_timeout_seconds,
                "maxDurationSeconds": max_duration_seconds,
                "backgroundSound": convert_and_respect_annotation_metadata(
                    object_=background_sound, annotation=CreateAssistantDtoBackgroundSound, direction="write"
                ),
                "backgroundDenoisingEnabled": background_denoising_enabled,
                "modelOutputInMessagesEnabled": model_output_in_messages_enabled,
                "transportConfigurations": convert_and_respect_annotation_metadata(
                    object_=transport_configurations,
                    annotation=typing.Sequence[TransportConfigurationTwilio],
                    direction="write",
                ),
                "observabilityPlan": convert_and_respect_annotation_metadata(
                    object_=observability_plan, annotation=LangfuseObservabilityPlan, direction="write"
                ),
                "credentials": convert_and_respect_annotation_metadata(
                    object_=credentials,
                    annotation=typing.Sequence[CreateAssistantDtoCredentialsItem],
                    direction="write",
                ),
                "hooks": convert_and_respect_annotation_metadata(
                    object_=hooks, annotation=typing.Sequence[CreateAssistantDtoHooksItem], direction="write"
                ),
                "name": name,
                "voicemailMessage": voicemail_message,
                "endCallMessage": end_call_message,
                "endCallPhrases": end_call_phrases,
                "compliancePlan": convert_and_respect_annotation_metadata(
                    object_=compliance_plan, annotation=CompliancePlan, direction="write"
                ),
                "metadata": metadata,
                "analysisPlan": convert_and_respect_annotation_metadata(
                    object_=analysis_plan, annotation=AnalysisPlan, direction="write"
                ),
                "artifactPlan": convert_and_respect_annotation_metadata(
                    object_=artifact_plan, annotation=ArtifactPlan, direction="write"
                ),
                "messagePlan": convert_and_respect_annotation_metadata(
                    object_=message_plan, annotation=MessagePlan, direction="write"
                ),
                "startSpeakingPlan": convert_and_respect_annotation_metadata(
                    object_=start_speaking_plan, annotation=StartSpeakingPlan, direction="write"
                ),
                "stopSpeakingPlan": convert_and_respect_annotation_metadata(
                    object_=stop_speaking_plan, annotation=StopSpeakingPlan, direction="write"
                ),
                "monitorPlan": convert_and_respect_annotation_metadata(
                    object_=monitor_plan, annotation=MonitorPlan, direction="write"
                ),
                "credentialIds": credential_ids,
                "server": convert_and_respect_annotation_metadata(object_=server, annotation=Server, direction="write"),
                "keypadInputPlan": convert_and_respect_annotation_metadata(
                    object_=keypad_input_plan, annotation=KeypadInputPlan, direction="write"
                ),
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    Assistant,
                    construct_type(
                        type_=Assistant,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)

    def get(self, id: str, *, request_options: typing.Optional[RequestOptions] = None) -> HttpResponse[Assistant]:
        """
        Parameters
        ----------
        id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[Assistant]

        """
        _response = self._client_wrapper.httpx_client.request(
            f"assistant/{jsonable_encoder(id)}",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    Assistant,
                    construct_type(
                        type_=Assistant,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)

    def delete(self, id: str, *, request_options: typing.Optional[RequestOptions] = None) -> HttpResponse[Assistant]:
        """
        Parameters
        ----------
        id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[Assistant]

        """
        _response = self._client_wrapper.httpx_client.request(
            f"assistant/{jsonable_encoder(id)}",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    Assistant,
                    construct_type(
                        type_=Assistant,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)

    def update(
        self,
        id: str,
        *,
        transcriber: typing.Optional[UpdateAssistantDtoTranscriber] = OMIT,
        model: typing.Optional[UpdateAssistantDtoModel] = OMIT,
        voice: typing.Optional[UpdateAssistantDtoVoice] = OMIT,
        first_message: typing.Optional[str] = OMIT,
        first_message_interruptions_enabled: typing.Optional[bool] = OMIT,
        first_message_mode: typing.Optional[UpdateAssistantDtoFirstMessageMode] = OMIT,
        voicemail_detection: typing.Optional[UpdateAssistantDtoVoicemailDetection] = OMIT,
        client_messages: typing.Optional[typing.Sequence[UpdateAssistantDtoClientMessagesItem]] = OMIT,
        server_messages: typing.Optional[typing.Sequence[UpdateAssistantDtoServerMessagesItem]] = OMIT,
        silence_timeout_seconds: typing.Optional[float] = OMIT,
        max_duration_seconds: typing.Optional[float] = OMIT,
        background_sound: typing.Optional[UpdateAssistantDtoBackgroundSound] = OMIT,
        background_denoising_enabled: typing.Optional[bool] = OMIT,
        model_output_in_messages_enabled: typing.Optional[bool] = OMIT,
        transport_configurations: typing.Optional[typing.Sequence[TransportConfigurationTwilio]] = OMIT,
        observability_plan: typing.Optional[LangfuseObservabilityPlan] = OMIT,
        credentials: typing.Optional[typing.Sequence[UpdateAssistantDtoCredentialsItem]] = OMIT,
        hooks: typing.Optional[typing.Sequence[UpdateAssistantDtoHooksItem]] = OMIT,
        name: typing.Optional[str] = OMIT,
        voicemail_message: typing.Optional[str] = OMIT,
        end_call_message: typing.Optional[str] = OMIT,
        end_call_phrases: typing.Optional[typing.Sequence[str]] = OMIT,
        compliance_plan: typing.Optional[CompliancePlan] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        analysis_plan: typing.Optional[AnalysisPlan] = OMIT,
        artifact_plan: typing.Optional[ArtifactPlan] = OMIT,
        message_plan: typing.Optional[MessagePlan] = OMIT,
        start_speaking_plan: typing.Optional[StartSpeakingPlan] = OMIT,
        stop_speaking_plan: typing.Optional[StopSpeakingPlan] = OMIT,
        monitor_plan: typing.Optional[MonitorPlan] = OMIT,
        credential_ids: typing.Optional[typing.Sequence[str]] = OMIT,
        server: typing.Optional[Server] = OMIT,
        keypad_input_plan: typing.Optional[KeypadInputPlan] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> HttpResponse[Assistant]:
        """
        Parameters
        ----------
        id : str

        transcriber : typing.Optional[UpdateAssistantDtoTranscriber]
            These are the options for the assistant's transcriber.

        model : typing.Optional[UpdateAssistantDtoModel]
            These are the options for the assistant's LLM.

        voice : typing.Optional[UpdateAssistantDtoVoice]
            These are the options for the assistant's voice.

        first_message : typing.Optional[str]
            This is the first message that the assistant will say. This can also be a URL to a containerized audio file (mp3, wav, etc.).

            If unspecified, assistant will wait for user to speak and use the model to respond once they speak.

        first_message_interruptions_enabled : typing.Optional[bool]

        first_message_mode : typing.Optional[UpdateAssistantDtoFirstMessageMode]
            This is the mode for the first message. Default is 'assistant-speaks-first'.

            Use:
            - 'assistant-speaks-first' to have the assistant speak first.
            - 'assistant-waits-for-user' to have the assistant wait for the user to speak first.
            - 'assistant-speaks-first-with-model-generated-message' to have the assistant speak first with a message generated by the model based on the conversation state. (`assistant.model.messages` at call start, `call.messages` at squad transfer points).

            @default 'assistant-speaks-first'

        voicemail_detection : typing.Optional[UpdateAssistantDtoVoicemailDetection]
            These are the settings to configure or disable voicemail detection. Alternatively, voicemail detection can be configured using the model.tools=[VoicemailTool].
            This uses Twilio's built-in detection while the VoicemailTool relies on the model to detect if a voicemail was reached.
            You can use neither of them, one of them, or both of them. By default, Twilio built-in detection is enabled while VoicemailTool is not.

        client_messages : typing.Optional[typing.Sequence[UpdateAssistantDtoClientMessagesItem]]
            These are the messages that will be sent to your Client SDKs. Default is conversation-update,function-call,hang,model-output,speech-update,status-update,transfer-update,transcript,tool-calls,user-interrupted,voice-input,workflow.node.started. You can check the shape of the messages in ClientMessage schema.

        server_messages : typing.Optional[typing.Sequence[UpdateAssistantDtoServerMessagesItem]]
            These are the messages that will be sent to your Server URL. Default is conversation-update,end-of-call-report,function-call,hang,speech-update,status-update,tool-calls,transfer-destination-request,user-interrupted. You can check the shape of the messages in ServerMessage schema.

        silence_timeout_seconds : typing.Optional[float]
            How many seconds of silence to wait before ending the call. Defaults to 30.

            @default 30

        max_duration_seconds : typing.Optional[float]
            This is the maximum number of seconds that the call will last. When the call reaches this duration, it will be ended.

            @default 600 (10 minutes)

        background_sound : typing.Optional[UpdateAssistantDtoBackgroundSound]
            This is the background sound in the call. Default for phone calls is 'office' and default for web calls is 'off'.
            You can also provide a custom sound by providing a URL to an audio file.

        background_denoising_enabled : typing.Optional[bool]
            This enables filtering of noise and background speech while the user is talking.

            Default `false` while in beta.

            @default false

        model_output_in_messages_enabled : typing.Optional[bool]
            This determines whether the model's output is used in conversation history rather than the transcription of assistant's speech.

            Default `false` while in beta.

            @default false

        transport_configurations : typing.Optional[typing.Sequence[TransportConfigurationTwilio]]
            These are the configurations to be passed to the transport providers of assistant's calls, like Twilio. You can store multiple configurations for different transport providers. For a call, only the configuration matching the call transport provider is used.

        observability_plan : typing.Optional[LangfuseObservabilityPlan]
            This is the plan for observability configuration of assistant's calls.
            Currently supports Langfuse for tracing and monitoring.

        credentials : typing.Optional[typing.Sequence[UpdateAssistantDtoCredentialsItem]]
            These are dynamic credentials that will be used for the assistant calls. By default, all the credentials are available for use in the call but you can supplement an additional credentials using this. Dynamic credentials override existing credentials.

        hooks : typing.Optional[typing.Sequence[UpdateAssistantDtoHooksItem]]
            This is a set of actions that will be performed on certain events.

        name : typing.Optional[str]
            This is the name of the assistant.

            This is required when you want to transfer between assistants in a call.

        voicemail_message : typing.Optional[str]
            This is the message that the assistant will say if the call is forwarded to voicemail.

            If unspecified, it will hang up.

        end_call_message : typing.Optional[str]
            This is the message that the assistant will say if it ends the call.

            If unspecified, it will hang up without saying anything.

        end_call_phrases : typing.Optional[typing.Sequence[str]]
            This list contains phrases that, if spoken by the assistant, will trigger the call to be hung up. Case insensitive.

        compliance_plan : typing.Optional[CompliancePlan]

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            This is for metadata you want to store on the assistant.

        analysis_plan : typing.Optional[AnalysisPlan]
            This is the plan for analysis of assistant's calls. Stored in `call.analysis`.

        artifact_plan : typing.Optional[ArtifactPlan]
            This is the plan for artifacts generated during assistant's calls. Stored in `call.artifact`.

            Note: `recordingEnabled` is currently at the root level. It will be moved to `artifactPlan` in the future, but will remain backwards compatible.

        message_plan : typing.Optional[MessagePlan]
            This is the plan for static predefined messages that can be spoken by the assistant during the call, like `idleMessages`.

            Note: `firstMessage`, `voicemailMessage`, and `endCallMessage` are currently at the root level. They will be moved to `messagePlan` in the future, but will remain backwards compatible.

        start_speaking_plan : typing.Optional[StartSpeakingPlan]
            This is the plan for when the assistant should start talking.

            You should configure this if you're running into these issues:
            - The assistant is too slow to start talking after the customer is done speaking.
            - The assistant is too fast to start talking after the customer is done speaking.
            - The assistant is so fast that it's actually interrupting the customer.

        stop_speaking_plan : typing.Optional[StopSpeakingPlan]
            This is the plan for when assistant should stop talking on customer interruption.

            You should configure this if you're running into these issues:
            - The assistant is too slow to recognize customer's interruption.
            - The assistant is too fast to recognize customer's interruption.
            - The assistant is getting interrupted by phrases that are just acknowledgments.
            - The assistant is getting interrupted by background noises.
            - The assistant is not properly stopping -- it starts talking right after getting interrupted.

        monitor_plan : typing.Optional[MonitorPlan]
            This is the plan for real-time monitoring of the assistant's calls.

            Usage:
            - To enable live listening of the assistant's calls, set `monitorPlan.listenEnabled` to `true`.
            - To enable live control of the assistant's calls, set `monitorPlan.controlEnabled` to `true`.

            Note, `serverMessages`, `clientMessages`, `serverUrl` and `serverUrlSecret` are currently at the root level but will be moved to `monitorPlan` in the future. Will remain backwards compatible

        credential_ids : typing.Optional[typing.Sequence[str]]
            These are the credentials that will be used for the assistant calls. By default, all the credentials are available for use in the call but you can provide a subset using this.

        server : typing.Optional[Server]
            This is where Vapi will send webhooks. You can find all webhooks available along with their shape in ServerMessage schema.

            The order of precedence is:

            1. assistant.server.url
            2. phoneNumber.serverUrl
            3. org.serverUrl

        keypad_input_plan : typing.Optional[KeypadInputPlan]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[Assistant]

        """
        _response = self._client_wrapper.httpx_client.request(
            f"assistant/{jsonable_encoder(id)}",
            method="PATCH",
            json={
                "transcriber": convert_and_respect_annotation_metadata(
                    object_=transcriber, annotation=UpdateAssistantDtoTranscriber, direction="write"
                ),
                "model": convert_and_respect_annotation_metadata(
                    object_=model, annotation=UpdateAssistantDtoModel, direction="write"
                ),
                "voice": convert_and_respect_annotation_metadata(
                    object_=voice, annotation=UpdateAssistantDtoVoice, direction="write"
                ),
                "firstMessage": first_message,
                "firstMessageInterruptionsEnabled": first_message_interruptions_enabled,
                "firstMessageMode": first_message_mode,
                "voicemailDetection": convert_and_respect_annotation_metadata(
                    object_=voicemail_detection, annotation=UpdateAssistantDtoVoicemailDetection, direction="write"
                ),
                "clientMessages": client_messages,
                "serverMessages": server_messages,
                "silenceTimeoutSeconds": silence_timeout_seconds,
                "maxDurationSeconds": max_duration_seconds,
                "backgroundSound": convert_and_respect_annotation_metadata(
                    object_=background_sound, annotation=UpdateAssistantDtoBackgroundSound, direction="write"
                ),
                "backgroundDenoisingEnabled": background_denoising_enabled,
                "modelOutputInMessagesEnabled": model_output_in_messages_enabled,
                "transportConfigurations": convert_and_respect_annotation_metadata(
                    object_=transport_configurations,
                    annotation=typing.Sequence[TransportConfigurationTwilio],
                    direction="write",
                ),
                "observabilityPlan": convert_and_respect_annotation_metadata(
                    object_=observability_plan, annotation=LangfuseObservabilityPlan, direction="write"
                ),
                "credentials": convert_and_respect_annotation_metadata(
                    object_=credentials,
                    annotation=typing.Sequence[UpdateAssistantDtoCredentialsItem],
                    direction="write",
                ),
                "hooks": convert_and_respect_annotation_metadata(
                    object_=hooks, annotation=typing.Sequence[UpdateAssistantDtoHooksItem], direction="write"
                ),
                "name": name,
                "voicemailMessage": voicemail_message,
                "endCallMessage": end_call_message,
                "endCallPhrases": end_call_phrases,
                "compliancePlan": convert_and_respect_annotation_metadata(
                    object_=compliance_plan, annotation=CompliancePlan, direction="write"
                ),
                "metadata": metadata,
                "analysisPlan": convert_and_respect_annotation_metadata(
                    object_=analysis_plan, annotation=AnalysisPlan, direction="write"
                ),
                "artifactPlan": convert_and_respect_annotation_metadata(
                    object_=artifact_plan, annotation=ArtifactPlan, direction="write"
                ),
                "messagePlan": convert_and_respect_annotation_metadata(
                    object_=message_plan, annotation=MessagePlan, direction="write"
                ),
                "startSpeakingPlan": convert_and_respect_annotation_metadata(
                    object_=start_speaking_plan, annotation=StartSpeakingPlan, direction="write"
                ),
                "stopSpeakingPlan": convert_and_respect_annotation_metadata(
                    object_=stop_speaking_plan, annotation=StopSpeakingPlan, direction="write"
                ),
                "monitorPlan": convert_and_respect_annotation_metadata(
                    object_=monitor_plan, annotation=MonitorPlan, direction="write"
                ),
                "credentialIds": credential_ids,
                "server": convert_and_respect_annotation_metadata(object_=server, annotation=Server, direction="write"),
                "keypadInputPlan": convert_and_respect_annotation_metadata(
                    object_=keypad_input_plan, annotation=KeypadInputPlan, direction="write"
                ),
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    Assistant,
                    construct_type(
                        type_=Assistant,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)


class AsyncRawAssistantsClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._client_wrapper = client_wrapper

    async def list(
        self,
        *,
        limit: typing.Optional[float] = None,
        created_at_gt: typing.Optional[dt.datetime] = None,
        created_at_lt: typing.Optional[dt.datetime] = None,
        created_at_ge: typing.Optional[dt.datetime] = None,
        created_at_le: typing.Optional[dt.datetime] = None,
        updated_at_gt: typing.Optional[dt.datetime] = None,
        updated_at_lt: typing.Optional[dt.datetime] = None,
        updated_at_ge: typing.Optional[dt.datetime] = None,
        updated_at_le: typing.Optional[dt.datetime] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncHttpResponse[typing.List[Assistant]]:
        """
        Parameters
        ----------
        limit : typing.Optional[float]
            This is the maximum number of items to return. Defaults to 100.

        created_at_gt : typing.Optional[dt.datetime]
            This will return items where the createdAt is greater than the specified value.

        created_at_lt : typing.Optional[dt.datetime]
            This will return items where the createdAt is less than the specified value.

        created_at_ge : typing.Optional[dt.datetime]
            This will return items where the createdAt is greater than or equal to the specified value.

        created_at_le : typing.Optional[dt.datetime]
            This will return items where the createdAt is less than or equal to the specified value.

        updated_at_gt : typing.Optional[dt.datetime]
            This will return items where the updatedAt is greater than the specified value.

        updated_at_lt : typing.Optional[dt.datetime]
            This will return items where the updatedAt is less than the specified value.

        updated_at_ge : typing.Optional[dt.datetime]
            This will return items where the updatedAt is greater than or equal to the specified value.

        updated_at_le : typing.Optional[dt.datetime]
            This will return items where the updatedAt is less than or equal to the specified value.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[typing.List[Assistant]]

        """
        _response = await self._client_wrapper.httpx_client.request(
            "assistant",
            method="GET",
            params={
                "limit": limit,
                "createdAtGt": serialize_datetime(created_at_gt) if created_at_gt is not None else None,
                "createdAtLt": serialize_datetime(created_at_lt) if created_at_lt is not None else None,
                "createdAtGe": serialize_datetime(created_at_ge) if created_at_ge is not None else None,
                "createdAtLe": serialize_datetime(created_at_le) if created_at_le is not None else None,
                "updatedAtGt": serialize_datetime(updated_at_gt) if updated_at_gt is not None else None,
                "updatedAtLt": serialize_datetime(updated_at_lt) if updated_at_lt is not None else None,
                "updatedAtGe": serialize_datetime(updated_at_ge) if updated_at_ge is not None else None,
                "updatedAtLe": serialize_datetime(updated_at_le) if updated_at_le is not None else None,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    typing.List[Assistant],
                    construct_type(
                        type_=typing.List[Assistant],  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)

    async def create(
        self,
        *,
        transcriber: typing.Optional[CreateAssistantDtoTranscriber] = OMIT,
        model: typing.Optional[CreateAssistantDtoModel] = OMIT,
        voice: typing.Optional[CreateAssistantDtoVoice] = OMIT,
        first_message: typing.Optional[str] = OMIT,
        first_message_interruptions_enabled: typing.Optional[bool] = OMIT,
        first_message_mode: typing.Optional[CreateAssistantDtoFirstMessageMode] = OMIT,
        voicemail_detection: typing.Optional[CreateAssistantDtoVoicemailDetection] = OMIT,
        client_messages: typing.Optional[typing.Sequence[CreateAssistantDtoClientMessagesItem]] = OMIT,
        server_messages: typing.Optional[typing.Sequence[CreateAssistantDtoServerMessagesItem]] = OMIT,
        silence_timeout_seconds: typing.Optional[float] = OMIT,
        max_duration_seconds: typing.Optional[float] = OMIT,
        background_sound: typing.Optional[CreateAssistantDtoBackgroundSound] = OMIT,
        background_denoising_enabled: typing.Optional[bool] = OMIT,
        model_output_in_messages_enabled: typing.Optional[bool] = OMIT,
        transport_configurations: typing.Optional[typing.Sequence[TransportConfigurationTwilio]] = OMIT,
        observability_plan: typing.Optional[LangfuseObservabilityPlan] = OMIT,
        credentials: typing.Optional[typing.Sequence[CreateAssistantDtoCredentialsItem]] = OMIT,
        hooks: typing.Optional[typing.Sequence[CreateAssistantDtoHooksItem]] = OMIT,
        name: typing.Optional[str] = OMIT,
        voicemail_message: typing.Optional[str] = OMIT,
        end_call_message: typing.Optional[str] = OMIT,
        end_call_phrases: typing.Optional[typing.Sequence[str]] = OMIT,
        compliance_plan: typing.Optional[CompliancePlan] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        analysis_plan: typing.Optional[AnalysisPlan] = OMIT,
        artifact_plan: typing.Optional[ArtifactPlan] = OMIT,
        message_plan: typing.Optional[MessagePlan] = OMIT,
        start_speaking_plan: typing.Optional[StartSpeakingPlan] = OMIT,
        stop_speaking_plan: typing.Optional[StopSpeakingPlan] = OMIT,
        monitor_plan: typing.Optional[MonitorPlan] = OMIT,
        credential_ids: typing.Optional[typing.Sequence[str]] = OMIT,
        server: typing.Optional[Server] = OMIT,
        keypad_input_plan: typing.Optional[KeypadInputPlan] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncHttpResponse[Assistant]:
        """
        Parameters
        ----------
        transcriber : typing.Optional[CreateAssistantDtoTranscriber]
            These are the options for the assistant's transcriber.

        model : typing.Optional[CreateAssistantDtoModel]
            These are the options for the assistant's LLM.

        voice : typing.Optional[CreateAssistantDtoVoice]
            These are the options for the assistant's voice.

        first_message : typing.Optional[str]
            This is the first message that the assistant will say. This can also be a URL to a containerized audio file (mp3, wav, etc.).

            If unspecified, assistant will wait for user to speak and use the model to respond once they speak.

        first_message_interruptions_enabled : typing.Optional[bool]

        first_message_mode : typing.Optional[CreateAssistantDtoFirstMessageMode]
            This is the mode for the first message. Default is 'assistant-speaks-first'.

            Use:
            - 'assistant-speaks-first' to have the assistant speak first.
            - 'assistant-waits-for-user' to have the assistant wait for the user to speak first.
            - 'assistant-speaks-first-with-model-generated-message' to have the assistant speak first with a message generated by the model based on the conversation state. (`assistant.model.messages` at call start, `call.messages` at squad transfer points).

            @default 'assistant-speaks-first'

        voicemail_detection : typing.Optional[CreateAssistantDtoVoicemailDetection]
            These are the settings to configure or disable voicemail detection. Alternatively, voicemail detection can be configured using the model.tools=[VoicemailTool].
            This uses Twilio's built-in detection while the VoicemailTool relies on the model to detect if a voicemail was reached.
            You can use neither of them, one of them, or both of them. By default, Twilio built-in detection is enabled while VoicemailTool is not.

        client_messages : typing.Optional[typing.Sequence[CreateAssistantDtoClientMessagesItem]]
            These are the messages that will be sent to your Client SDKs. Default is conversation-update,function-call,hang,model-output,speech-update,status-update,transfer-update,transcript,tool-calls,user-interrupted,voice-input,workflow.node.started. You can check the shape of the messages in ClientMessage schema.

        server_messages : typing.Optional[typing.Sequence[CreateAssistantDtoServerMessagesItem]]
            These are the messages that will be sent to your Server URL. Default is conversation-update,end-of-call-report,function-call,hang,speech-update,status-update,tool-calls,transfer-destination-request,user-interrupted. You can check the shape of the messages in ServerMessage schema.

        silence_timeout_seconds : typing.Optional[float]
            How many seconds of silence to wait before ending the call. Defaults to 30.

            @default 30

        max_duration_seconds : typing.Optional[float]
            This is the maximum number of seconds that the call will last. When the call reaches this duration, it will be ended.

            @default 600 (10 minutes)

        background_sound : typing.Optional[CreateAssistantDtoBackgroundSound]
            This is the background sound in the call. Default for phone calls is 'office' and default for web calls is 'off'.
            You can also provide a custom sound by providing a URL to an audio file.

        background_denoising_enabled : typing.Optional[bool]
            This enables filtering of noise and background speech while the user is talking.

            Default `false` while in beta.

            @default false

        model_output_in_messages_enabled : typing.Optional[bool]
            This determines whether the model's output is used in conversation history rather than the transcription of assistant's speech.

            Default `false` while in beta.

            @default false

        transport_configurations : typing.Optional[typing.Sequence[TransportConfigurationTwilio]]
            These are the configurations to be passed to the transport providers of assistant's calls, like Twilio. You can store multiple configurations for different transport providers. For a call, only the configuration matching the call transport provider is used.

        observability_plan : typing.Optional[LangfuseObservabilityPlan]
            This is the plan for observability configuration of assistant's calls.
            Currently supports Langfuse for tracing and monitoring.

        credentials : typing.Optional[typing.Sequence[CreateAssistantDtoCredentialsItem]]
            These are dynamic credentials that will be used for the assistant calls. By default, all the credentials are available for use in the call but you can supplement an additional credentials using this. Dynamic credentials override existing credentials.

        hooks : typing.Optional[typing.Sequence[CreateAssistantDtoHooksItem]]
            This is a set of actions that will be performed on certain events.

        name : typing.Optional[str]
            This is the name of the assistant.

            This is required when you want to transfer between assistants in a call.

        voicemail_message : typing.Optional[str]
            This is the message that the assistant will say if the call is forwarded to voicemail.

            If unspecified, it will hang up.

        end_call_message : typing.Optional[str]
            This is the message that the assistant will say if it ends the call.

            If unspecified, it will hang up without saying anything.

        end_call_phrases : typing.Optional[typing.Sequence[str]]
            This list contains phrases that, if spoken by the assistant, will trigger the call to be hung up. Case insensitive.

        compliance_plan : typing.Optional[CompliancePlan]

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            This is for metadata you want to store on the assistant.

        analysis_plan : typing.Optional[AnalysisPlan]
            This is the plan for analysis of assistant's calls. Stored in `call.analysis`.

        artifact_plan : typing.Optional[ArtifactPlan]
            This is the plan for artifacts generated during assistant's calls. Stored in `call.artifact`.

            Note: `recordingEnabled` is currently at the root level. It will be moved to `artifactPlan` in the future, but will remain backwards compatible.

        message_plan : typing.Optional[MessagePlan]
            This is the plan for static predefined messages that can be spoken by the assistant during the call, like `idleMessages`.

            Note: `firstMessage`, `voicemailMessage`, and `endCallMessage` are currently at the root level. They will be moved to `messagePlan` in the future, but will remain backwards compatible.

        start_speaking_plan : typing.Optional[StartSpeakingPlan]
            This is the plan for when the assistant should start talking.

            You should configure this if you're running into these issues:
            - The assistant is too slow to start talking after the customer is done speaking.
            - The assistant is too fast to start talking after the customer is done speaking.
            - The assistant is so fast that it's actually interrupting the customer.

        stop_speaking_plan : typing.Optional[StopSpeakingPlan]
            This is the plan for when assistant should stop talking on customer interruption.

            You should configure this if you're running into these issues:
            - The assistant is too slow to recognize customer's interruption.
            - The assistant is too fast to recognize customer's interruption.
            - The assistant is getting interrupted by phrases that are just acknowledgments.
            - The assistant is getting interrupted by background noises.
            - The assistant is not properly stopping -- it starts talking right after getting interrupted.

        monitor_plan : typing.Optional[MonitorPlan]
            This is the plan for real-time monitoring of the assistant's calls.

            Usage:
            - To enable live listening of the assistant's calls, set `monitorPlan.listenEnabled` to `true`.
            - To enable live control of the assistant's calls, set `monitorPlan.controlEnabled` to `true`.

            Note, `serverMessages`, `clientMessages`, `serverUrl` and `serverUrlSecret` are currently at the root level but will be moved to `monitorPlan` in the future. Will remain backwards compatible

        credential_ids : typing.Optional[typing.Sequence[str]]
            These are the credentials that will be used for the assistant calls. By default, all the credentials are available for use in the call but you can provide a subset using this.

        server : typing.Optional[Server]
            This is where Vapi will send webhooks. You can find all webhooks available along with their shape in ServerMessage schema.

            The order of precedence is:

            1. assistant.server.url
            2. phoneNumber.serverUrl
            3. org.serverUrl

        keypad_input_plan : typing.Optional[KeypadInputPlan]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[Assistant]

        """
        _response = await self._client_wrapper.httpx_client.request(
            "assistant",
            method="POST",
            json={
                "transcriber": convert_and_respect_annotation_metadata(
                    object_=transcriber, annotation=CreateAssistantDtoTranscriber, direction="write"
                ),
                "model": convert_and_respect_annotation_metadata(
                    object_=model, annotation=CreateAssistantDtoModel, direction="write"
                ),
                "voice": convert_and_respect_annotation_metadata(
                    object_=voice, annotation=CreateAssistantDtoVoice, direction="write"
                ),
                "firstMessage": first_message,
                "firstMessageInterruptionsEnabled": first_message_interruptions_enabled,
                "firstMessageMode": first_message_mode,
                "voicemailDetection": convert_and_respect_annotation_metadata(
                    object_=voicemail_detection, annotation=CreateAssistantDtoVoicemailDetection, direction="write"
                ),
                "clientMessages": client_messages,
                "serverMessages": server_messages,
                "silenceTimeoutSeconds": silence_timeout_seconds,
                "maxDurationSeconds": max_duration_seconds,
                "backgroundSound": convert_and_respect_annotation_metadata(
                    object_=background_sound, annotation=CreateAssistantDtoBackgroundSound, direction="write"
                ),
                "backgroundDenoisingEnabled": background_denoising_enabled,
                "modelOutputInMessagesEnabled": model_output_in_messages_enabled,
                "transportConfigurations": convert_and_respect_annotation_metadata(
                    object_=transport_configurations,
                    annotation=typing.Sequence[TransportConfigurationTwilio],
                    direction="write",
                ),
                "observabilityPlan": convert_and_respect_annotation_metadata(
                    object_=observability_plan, annotation=LangfuseObservabilityPlan, direction="write"
                ),
                "credentials": convert_and_respect_annotation_metadata(
                    object_=credentials,
                    annotation=typing.Sequence[CreateAssistantDtoCredentialsItem],
                    direction="write",
                ),
                "hooks": convert_and_respect_annotation_metadata(
                    object_=hooks, annotation=typing.Sequence[CreateAssistantDtoHooksItem], direction="write"
                ),
                "name": name,
                "voicemailMessage": voicemail_message,
                "endCallMessage": end_call_message,
                "endCallPhrases": end_call_phrases,
                "compliancePlan": convert_and_respect_annotation_metadata(
                    object_=compliance_plan, annotation=CompliancePlan, direction="write"
                ),
                "metadata": metadata,
                "analysisPlan": convert_and_respect_annotation_metadata(
                    object_=analysis_plan, annotation=AnalysisPlan, direction="write"
                ),
                "artifactPlan": convert_and_respect_annotation_metadata(
                    object_=artifact_plan, annotation=ArtifactPlan, direction="write"
                ),
                "messagePlan": convert_and_respect_annotation_metadata(
                    object_=message_plan, annotation=MessagePlan, direction="write"
                ),
                "startSpeakingPlan": convert_and_respect_annotation_metadata(
                    object_=start_speaking_plan, annotation=StartSpeakingPlan, direction="write"
                ),
                "stopSpeakingPlan": convert_and_respect_annotation_metadata(
                    object_=stop_speaking_plan, annotation=StopSpeakingPlan, direction="write"
                ),
                "monitorPlan": convert_and_respect_annotation_metadata(
                    object_=monitor_plan, annotation=MonitorPlan, direction="write"
                ),
                "credentialIds": credential_ids,
                "server": convert_and_respect_annotation_metadata(object_=server, annotation=Server, direction="write"),
                "keypadInputPlan": convert_and_respect_annotation_metadata(
                    object_=keypad_input_plan, annotation=KeypadInputPlan, direction="write"
                ),
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    Assistant,
                    construct_type(
                        type_=Assistant,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)

    async def get(
        self, id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> AsyncHttpResponse[Assistant]:
        """
        Parameters
        ----------
        id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[Assistant]

        """
        _response = await self._client_wrapper.httpx_client.request(
            f"assistant/{jsonable_encoder(id)}",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    Assistant,
                    construct_type(
                        type_=Assistant,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)

    async def delete(
        self, id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> AsyncHttpResponse[Assistant]:
        """
        Parameters
        ----------
        id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[Assistant]

        """
        _response = await self._client_wrapper.httpx_client.request(
            f"assistant/{jsonable_encoder(id)}",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    Assistant,
                    construct_type(
                        type_=Assistant,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)

    async def update(
        self,
        id: str,
        *,
        transcriber: typing.Optional[UpdateAssistantDtoTranscriber] = OMIT,
        model: typing.Optional[UpdateAssistantDtoModel] = OMIT,
        voice: typing.Optional[UpdateAssistantDtoVoice] = OMIT,
        first_message: typing.Optional[str] = OMIT,
        first_message_interruptions_enabled: typing.Optional[bool] = OMIT,
        first_message_mode: typing.Optional[UpdateAssistantDtoFirstMessageMode] = OMIT,
        voicemail_detection: typing.Optional[UpdateAssistantDtoVoicemailDetection] = OMIT,
        client_messages: typing.Optional[typing.Sequence[UpdateAssistantDtoClientMessagesItem]] = OMIT,
        server_messages: typing.Optional[typing.Sequence[UpdateAssistantDtoServerMessagesItem]] = OMIT,
        silence_timeout_seconds: typing.Optional[float] = OMIT,
        max_duration_seconds: typing.Optional[float] = OMIT,
        background_sound: typing.Optional[UpdateAssistantDtoBackgroundSound] = OMIT,
        background_denoising_enabled: typing.Optional[bool] = OMIT,
        model_output_in_messages_enabled: typing.Optional[bool] = OMIT,
        transport_configurations: typing.Optional[typing.Sequence[TransportConfigurationTwilio]] = OMIT,
        observability_plan: typing.Optional[LangfuseObservabilityPlan] = OMIT,
        credentials: typing.Optional[typing.Sequence[UpdateAssistantDtoCredentialsItem]] = OMIT,
        hooks: typing.Optional[typing.Sequence[UpdateAssistantDtoHooksItem]] = OMIT,
        name: typing.Optional[str] = OMIT,
        voicemail_message: typing.Optional[str] = OMIT,
        end_call_message: typing.Optional[str] = OMIT,
        end_call_phrases: typing.Optional[typing.Sequence[str]] = OMIT,
        compliance_plan: typing.Optional[CompliancePlan] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        analysis_plan: typing.Optional[AnalysisPlan] = OMIT,
        artifact_plan: typing.Optional[ArtifactPlan] = OMIT,
        message_plan: typing.Optional[MessagePlan] = OMIT,
        start_speaking_plan: typing.Optional[StartSpeakingPlan] = OMIT,
        stop_speaking_plan: typing.Optional[StopSpeakingPlan] = OMIT,
        monitor_plan: typing.Optional[MonitorPlan] = OMIT,
        credential_ids: typing.Optional[typing.Sequence[str]] = OMIT,
        server: typing.Optional[Server] = OMIT,
        keypad_input_plan: typing.Optional[KeypadInputPlan] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncHttpResponse[Assistant]:
        """
        Parameters
        ----------
        id : str

        transcriber : typing.Optional[UpdateAssistantDtoTranscriber]
            These are the options for the assistant's transcriber.

        model : typing.Optional[UpdateAssistantDtoModel]
            These are the options for the assistant's LLM.

        voice : typing.Optional[UpdateAssistantDtoVoice]
            These are the options for the assistant's voice.

        first_message : typing.Optional[str]
            This is the first message that the assistant will say. This can also be a URL to a containerized audio file (mp3, wav, etc.).

            If unspecified, assistant will wait for user to speak and use the model to respond once they speak.

        first_message_interruptions_enabled : typing.Optional[bool]

        first_message_mode : typing.Optional[UpdateAssistantDtoFirstMessageMode]
            This is the mode for the first message. Default is 'assistant-speaks-first'.

            Use:
            - 'assistant-speaks-first' to have the assistant speak first.
            - 'assistant-waits-for-user' to have the assistant wait for the user to speak first.
            - 'assistant-speaks-first-with-model-generated-message' to have the assistant speak first with a message generated by the model based on the conversation state. (`assistant.model.messages` at call start, `call.messages` at squad transfer points).

            @default 'assistant-speaks-first'

        voicemail_detection : typing.Optional[UpdateAssistantDtoVoicemailDetection]
            These are the settings to configure or disable voicemail detection. Alternatively, voicemail detection can be configured using the model.tools=[VoicemailTool].
            This uses Twilio's built-in detection while the VoicemailTool relies on the model to detect if a voicemail was reached.
            You can use neither of them, one of them, or both of them. By default, Twilio built-in detection is enabled while VoicemailTool is not.

        client_messages : typing.Optional[typing.Sequence[UpdateAssistantDtoClientMessagesItem]]
            These are the messages that will be sent to your Client SDKs. Default is conversation-update,function-call,hang,model-output,speech-update,status-update,transfer-update,transcript,tool-calls,user-interrupted,voice-input,workflow.node.started. You can check the shape of the messages in ClientMessage schema.

        server_messages : typing.Optional[typing.Sequence[UpdateAssistantDtoServerMessagesItem]]
            These are the messages that will be sent to your Server URL. Default is conversation-update,end-of-call-report,function-call,hang,speech-update,status-update,tool-calls,transfer-destination-request,user-interrupted. You can check the shape of the messages in ServerMessage schema.

        silence_timeout_seconds : typing.Optional[float]
            How many seconds of silence to wait before ending the call. Defaults to 30.

            @default 30

        max_duration_seconds : typing.Optional[float]
            This is the maximum number of seconds that the call will last. When the call reaches this duration, it will be ended.

            @default 600 (10 minutes)

        background_sound : typing.Optional[UpdateAssistantDtoBackgroundSound]
            This is the background sound in the call. Default for phone calls is 'office' and default for web calls is 'off'.
            You can also provide a custom sound by providing a URL to an audio file.

        background_denoising_enabled : typing.Optional[bool]
            This enables filtering of noise and background speech while the user is talking.

            Default `false` while in beta.

            @default false

        model_output_in_messages_enabled : typing.Optional[bool]
            This determines whether the model's output is used in conversation history rather than the transcription of assistant's speech.

            Default `false` while in beta.

            @default false

        transport_configurations : typing.Optional[typing.Sequence[TransportConfigurationTwilio]]
            These are the configurations to be passed to the transport providers of assistant's calls, like Twilio. You can store multiple configurations for different transport providers. For a call, only the configuration matching the call transport provider is used.

        observability_plan : typing.Optional[LangfuseObservabilityPlan]
            This is the plan for observability configuration of assistant's calls.
            Currently supports Langfuse for tracing and monitoring.

        credentials : typing.Optional[typing.Sequence[UpdateAssistantDtoCredentialsItem]]
            These are dynamic credentials that will be used for the assistant calls. By default, all the credentials are available for use in the call but you can supplement an additional credentials using this. Dynamic credentials override existing credentials.

        hooks : typing.Optional[typing.Sequence[UpdateAssistantDtoHooksItem]]
            This is a set of actions that will be performed on certain events.

        name : typing.Optional[str]
            This is the name of the assistant.

            This is required when you want to transfer between assistants in a call.

        voicemail_message : typing.Optional[str]
            This is the message that the assistant will say if the call is forwarded to voicemail.

            If unspecified, it will hang up.

        end_call_message : typing.Optional[str]
            This is the message that the assistant will say if it ends the call.

            If unspecified, it will hang up without saying anything.

        end_call_phrases : typing.Optional[typing.Sequence[str]]
            This list contains phrases that, if spoken by the assistant, will trigger the call to be hung up. Case insensitive.

        compliance_plan : typing.Optional[CompliancePlan]

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]
            This is for metadata you want to store on the assistant.

        analysis_plan : typing.Optional[AnalysisPlan]
            This is the plan for analysis of assistant's calls. Stored in `call.analysis`.

        artifact_plan : typing.Optional[ArtifactPlan]
            This is the plan for artifacts generated during assistant's calls. Stored in `call.artifact`.

            Note: `recordingEnabled` is currently at the root level. It will be moved to `artifactPlan` in the future, but will remain backwards compatible.

        message_plan : typing.Optional[MessagePlan]
            This is the plan for static predefined messages that can be spoken by the assistant during the call, like `idleMessages`.

            Note: `firstMessage`, `voicemailMessage`, and `endCallMessage` are currently at the root level. They will be moved to `messagePlan` in the future, but will remain backwards compatible.

        start_speaking_plan : typing.Optional[StartSpeakingPlan]
            This is the plan for when the assistant should start talking.

            You should configure this if you're running into these issues:
            - The assistant is too slow to start talking after the customer is done speaking.
            - The assistant is too fast to start talking after the customer is done speaking.
            - The assistant is so fast that it's actually interrupting the customer.

        stop_speaking_plan : typing.Optional[StopSpeakingPlan]
            This is the plan for when assistant should stop talking on customer interruption.

            You should configure this if you're running into these issues:
            - The assistant is too slow to recognize customer's interruption.
            - The assistant is too fast to recognize customer's interruption.
            - The assistant is getting interrupted by phrases that are just acknowledgments.
            - The assistant is getting interrupted by background noises.
            - The assistant is not properly stopping -- it starts talking right after getting interrupted.

        monitor_plan : typing.Optional[MonitorPlan]
            This is the plan for real-time monitoring of the assistant's calls.

            Usage:
            - To enable live listening of the assistant's calls, set `monitorPlan.listenEnabled` to `true`.
            - To enable live control of the assistant's calls, set `monitorPlan.controlEnabled` to `true`.

            Note, `serverMessages`, `clientMessages`, `serverUrl` and `serverUrlSecret` are currently at the root level but will be moved to `monitorPlan` in the future. Will remain backwards compatible

        credential_ids : typing.Optional[typing.Sequence[str]]
            These are the credentials that will be used for the assistant calls. By default, all the credentials are available for use in the call but you can provide a subset using this.

        server : typing.Optional[Server]
            This is where Vapi will send webhooks. You can find all webhooks available along with their shape in ServerMessage schema.

            The order of precedence is:

            1. assistant.server.url
            2. phoneNumber.serverUrl
            3. org.serverUrl

        keypad_input_plan : typing.Optional[KeypadInputPlan]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[Assistant]

        """
        _response = await self._client_wrapper.httpx_client.request(
            f"assistant/{jsonable_encoder(id)}",
            method="PATCH",
            json={
                "transcriber": convert_and_respect_annotation_metadata(
                    object_=transcriber, annotation=UpdateAssistantDtoTranscriber, direction="write"
                ),
                "model": convert_and_respect_annotation_metadata(
                    object_=model, annotation=UpdateAssistantDtoModel, direction="write"
                ),
                "voice": convert_and_respect_annotation_metadata(
                    object_=voice, annotation=UpdateAssistantDtoVoice, direction="write"
                ),
                "firstMessage": first_message,
                "firstMessageInterruptionsEnabled": first_message_interruptions_enabled,
                "firstMessageMode": first_message_mode,
                "voicemailDetection": convert_and_respect_annotation_metadata(
                    object_=voicemail_detection, annotation=UpdateAssistantDtoVoicemailDetection, direction="write"
                ),
                "clientMessages": client_messages,
                "serverMessages": server_messages,
                "silenceTimeoutSeconds": silence_timeout_seconds,
                "maxDurationSeconds": max_duration_seconds,
                "backgroundSound": convert_and_respect_annotation_metadata(
                    object_=background_sound, annotation=UpdateAssistantDtoBackgroundSound, direction="write"
                ),
                "backgroundDenoisingEnabled": background_denoising_enabled,
                "modelOutputInMessagesEnabled": model_output_in_messages_enabled,
                "transportConfigurations": convert_and_respect_annotation_metadata(
                    object_=transport_configurations,
                    annotation=typing.Sequence[TransportConfigurationTwilio],
                    direction="write",
                ),
                "observabilityPlan": convert_and_respect_annotation_metadata(
                    object_=observability_plan, annotation=LangfuseObservabilityPlan, direction="write"
                ),
                "credentials": convert_and_respect_annotation_metadata(
                    object_=credentials,
                    annotation=typing.Sequence[UpdateAssistantDtoCredentialsItem],
                    direction="write",
                ),
                "hooks": convert_and_respect_annotation_metadata(
                    object_=hooks, annotation=typing.Sequence[UpdateAssistantDtoHooksItem], direction="write"
                ),
                "name": name,
                "voicemailMessage": voicemail_message,
                "endCallMessage": end_call_message,
                "endCallPhrases": end_call_phrases,
                "compliancePlan": convert_and_respect_annotation_metadata(
                    object_=compliance_plan, annotation=CompliancePlan, direction="write"
                ),
                "metadata": metadata,
                "analysisPlan": convert_and_respect_annotation_metadata(
                    object_=analysis_plan, annotation=AnalysisPlan, direction="write"
                ),
                "artifactPlan": convert_and_respect_annotation_metadata(
                    object_=artifact_plan, annotation=ArtifactPlan, direction="write"
                ),
                "messagePlan": convert_and_respect_annotation_metadata(
                    object_=message_plan, annotation=MessagePlan, direction="write"
                ),
                "startSpeakingPlan": convert_and_respect_annotation_metadata(
                    object_=start_speaking_plan, annotation=StartSpeakingPlan, direction="write"
                ),
                "stopSpeakingPlan": convert_and_respect_annotation_metadata(
                    object_=stop_speaking_plan, annotation=StopSpeakingPlan, direction="write"
                ),
                "monitorPlan": convert_and_respect_annotation_metadata(
                    object_=monitor_plan, annotation=MonitorPlan, direction="write"
                ),
                "credentialIds": credential_ids,
                "server": convert_and_respect_annotation_metadata(object_=server, annotation=Server, direction="write"),
                "keypadInputPlan": convert_and_respect_annotation_metadata(
                    object_=keypad_input_plan, annotation=KeypadInputPlan, direction="write"
                ),
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    Assistant,
                    construct_type(
                        type_=Assistant,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)
